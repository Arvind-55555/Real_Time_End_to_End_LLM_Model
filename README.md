# ğŸš€ Real Time End to End LLM Model

This repository provides a **production-ready pipeline** for building, deploying, and running a **real-time end-to-end Large Language Model (LLM) system**.

It integrates **data ingestion, embedding, semantic retrieval (RAG), and text generation** into one deployable and scalable service with Docker and Kubernetes support.

---

## ğŸ§© Repository Structure

```
Real_Time_End_to_End_LLM_Model/
â”œâ”€â”€ app/                        # Core application logic
â”‚   â”œâ”€â”€ api.py                  # FastAPI routes for ingest, query, answer (RAG API)
â”‚   â”œâ”€â”€ main.py                 # Entry point for running the API service
â”‚   â”œâ”€â”€ ingest.py               # Document ingestion & embedding pipeline
â”‚   â”œâ”€â”€ embeddings.py           # Handles embedding generation via SentenceTransformers
â”‚   â”œâ”€â”€ retriever.py            # Chroma-based semantic search retriever
â”‚   â”œâ”€â”€ generator.py            # LLM text generation (GPT2 or HF model)
â”‚   â”œâ”€â”€ config.py               # Environment variables and global settings
â”‚   â””â”€â”€ utils.py                # Helper functions (if needed)
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ docker-compose.yml      # Docker Compose setup (app + Kafka + Chroma DB)
â”‚   â”œâ”€â”€ Dockerfile              # Docker image for API service
â”‚   â””â”€â”€ k8s/                    # Kubernetes deployment manifests
â”‚       â”œâ”€â”€ deployment.yaml     # API deployment spec
â”‚       â”œâ”€â”€ service.yaml        # K8s LoadBalancer service
â”‚       â””â”€â”€ chroma-pvc.yaml     # Persistent volume for Chroma DB
â”‚
â”œâ”€â”€ producers/
â”‚   â””â”€â”€ kafka_producer_sim.py   # Simulated Kafka producer sending live text data
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_api.py             # Unit tests for API endpoints
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yml                  # CI/CD pipeline for testing and Docker image build
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # (this file)
```

---

## âš™ï¸ Features

âœ… **Real-time ingestion** â€” uses Kafka for continuous data streaming  
âœ… **Embeddings** â€” powered by SentenceTransformers  
âœ… **Semantic Search** â€” persistent vector database (Chroma)  
âœ… **RAG (Retrieval-Augmented Generation)** â€” context-aware LLM answering  
âœ… **FastAPI microservice** â€” production-grade REST API  
âœ… **Docker & Kubernetes** â€” containerized & deployable stack  
âœ… **CI/CD** â€” GitHub Actions pipeline for build & test  
âœ… **Scalable & extensible** â€” easily plug in Weaviate/Milvus or Ollama/OpenAI models  

---

## ğŸ§  Quick Start

### 1ï¸âƒ£ Local setup (Docker Compose)
```bash
docker compose -f infra/docker-compose.yml up --build
```

### 2ï¸âƒ£ Send data for ingestion
```bash
curl -XPOST "http://localhost:8080/ingest" -H "Content-Type: application/json" -d '[{"id":"1","text":"AI is revolutionizing industries."}]'
```

### 3ï¸âƒ£ Query or generate
```bash
curl -XPOST "http://localhost:8080/answer" -H "Content-Type: application/json" -d '{"query":"What is AI?"}'
```

---

## ğŸš€ Deployment Options

- **Local development** â€” via Docker Compose
- **Production** â€” deploy using Kubernetes manifests (`infra/k8s/`)
- **Cloud-native** â€” integrate with managed vector stores (Weaviate/Milvus) and hosted LLMs

---

## ğŸ§° Technologies Used

- **Python 3.10**
- **FastAPI + Uvicorn**
- **Kafka (real-time ingestion)**
- **SentenceTransformers**
- **ChromaDB / FAISS**
- **Transformers (Hugging Face)**
- **Docker, Kubernetes**
- **GitHub Actions (CI/CD)**

---

